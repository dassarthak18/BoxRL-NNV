@misc{onnx,
  author = {ONNX},
  title = {Open Neural Networks Exchange: The open standard for machine learning interoperability},
  year = {2017},
  howpublished = {\url{https://onnx.ai/}},
  note = {Official Website}
  }

@inproceedings{FoMLAS2023:Supporting_Standardization_Neural_Networks,
  author    = {Stefano Demarchi and Dario Guidotti and Luca Pulina and Armando Tacchella},
  title     = {Supporting Standardization of Neural Networks Verification with VNNLIB and CoCoNet},
  booktitle = {Proceedings of the 6th Workshop on Formal Methods for ML-Enabled Autonomous Systems},
  editor    = {Nina Narodytska and Guy Amir and Guy Katz and Omri Isac},
  series    = {Kalpa Publications in Computing},
  volume    = {16},
  publisher = {EasyChair},
  bibsource = {EasyChair, https://easychair.org},
  issn      = {2515-1762},
  url       = {/publications/paper/Qgdn},
  doi       = {10.29007/5pdh},
  pages     = {47-58},
  year      = {2023}}

@misc{yang2018deepneuraldecisiontrees,
      title={Deep Neural Decision Trees}, 
      author={Yongxin Yang and Irene Garcia Morillo and Timothy M. Hospedales},
      year={2018},
      eprint={1806.06988},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1806.06988}, 
}

@InProceedings{pmlr-v216-li23c,
  title = 	 {{G}aussian Process Surrogate Models for Neural Networks},
  author =       {Li, Michael Y. and Grant, Erin and Griffiths, Thomas L.},
  booktitle = 	 {Proceedings of the Thirty-Ninth Conference on Uncertainty in Artificial Intelligence},
  pages = 	 {1241--1252},
  year = 	 {2023},
  editor = 	 {Evans, Robin J. and Shpitser, Ilya},
  volume = 	 {216},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {31 Jul--04 Aug},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v216/li23c/li23c.pdf},
  url = 	 {https://proceedings.mlr.press/v216/li23c.html},
  abstract = 	 {Not being able to understand and predict the behavior of deep learning systems makes it hard to decide what architecture and algorithm to use for a given problem. In science and engineering, modeling is a methodology used to understand complex systems whose internal processes are opaque. Modeling replaces a complex system with a simpler, more interpretable surrogate. Drawing inspiration from this, we construct a class of surrogate models for neural networks using Gaussian processes. Rather than deriving kernels for infinite neural networks, we learn kernels empirically from the naturalistic behavior of finite neural networks. We demonstrate our approach captures existing phenomena related to the spectral bias of neural networks, and then show that our surrogate models can be used to solve practical problems such as identifying which points most influence the behavior of specific neural networks and predicting which architectures and algorithms will generalize well for specific datasets.}
}

@inproceedings{ribeiro2016should,
  title={" Why should i trust you?" Explaining the predictions of any classifier},
  author={Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  booktitle={Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining},
  pages={1135--1144},
  year={2016}
}

@misc{lundberg2017unifiedapproachinterpretingmodel,
      title={A Unified Approach to Interpreting Model Predictions}, 
      author={Scott Lundberg and Su-In Lee},
      year={2017},
      eprint={1705.07874},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/1705.07874}, 
}

@article{zhang2021survey,
  title={A survey on neural network interpretability},
  author={Zhang, Yu and Ti{\v{n}}o, Peter and Leonardis, Ale{\v{s}} and Tang, Ke},
  journal={IEEE Transactions on Emerging Topics in Computational Intelligence},
  volume={5},
  number={5},
  pages={726--742},
  year={2021},
  publisher={IEEE}
}

@misc{brix2023fourthinternationalverificationneural,
      title={The Fourth International Verification of Neural Networks Competition (VNN-COMP 2023): Summary and Results}, 
      author={Christopher Brix and Stanley Bak and Changliu Liu and Taylor T. Johnson},
      year={2023},
      eprint={2312.16760},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2312.16760}, 
}

@inproceedings{wu2024marabou,
  title={Marabou 2.0: a versatile formal analyzer of neural networks},
  author={Wu, Haoze and Isac, Omri and Zelji{\'c}, Aleksandar and Tagomori, Teruhiro and Daggitt, Matthew and Kokke, Wen and Refaeli, Idan and Amir, Guy and Julian, Kyle and Bassan, Shahaf and others},
  booktitle={International Conference on Computer Aided Verification},
  pages={249--264},
  year={2024},
  organization={Springer}
}

@misc{abcrown,
  author = {Huan Zhang},
  title = {$\alpha,\beta$-CROWN (alpha-beta-CROWN): A Fast and Scalable Neural Network Verifier with Efficient Bound Propagation},
  year = {2018},
  howpublished = {\url{https://github.com/Verified-Intelligence/alpha-beta-CROWN}},
  note = {GitHub Repository}
  }

@article{duong2023dpll,
  title={A dpll (t) framework for verifying deep neural networks},
  author={Duong, Hai and Nguyen, ThanhVu and Dwyer, Matthew},
  journal={arXiv preprint arXiv:2307.10266},
  year={2023}
}

@article{ef76b040-2f28-37ba-b0c4-02ed99573416,
 ISSN = {00401706},
 URL = {http://www.jstor.org/stable/1268522},
 abstract = {Two types of sampling plans are examined as alternatives to simple random sampling in Monte Carlo studies. These plans are shown to be improvements over simple random sampling with respect to variance for a class of estimators which includes the sample mean and the empirical distribution function.},
 author = {M. D. McKay and R. J. Beckman and W. J. Conover},
 journal = {Technometrics},
 number = {2},
 pages = {239--245},
 publisher = {[Taylor & Francis, Ltd., American Statistical Association, American Society for Quality]},
 title = {A Comparison of Three Methods for Selecting Values of Input Variables in the Analysis of Output from a Computer Code},
 urldate = {2024-12-05},
 volume = {21},
 year = {1979}
}

@article{doi:10.1137/0916069,
author = {Byrd, Richard H. and Lu, Peihuang and Nocedal, Jorge and Zhu, Ciyou},
title = {A Limited Memory Algorithm for Bound Constrained Optimization},
journal = {SIAM Journal on Scientific Computing},
volume = {16},
number = {5},
pages = {1190-1208},
year = {1995},
doi = {10.1137/0916069},

URL = {https://doi.org/10.1137/0916069},
eprint = {https://doi.org/10.1137/0916069},
    abstract = { An algorithm for solving large nonlinear optimization problems with simple bounds is described. It is based on the gradient projection method and uses a limited memory BFGS matrix to approximate the Hessian of the objective function. It is shown how to take advantage of the form of the limited memory approximation to implement the algorithm efficiently. The results of numerical tests on a set of large problems are reported. }
}

@InProceedings{10.1007/978-3-540-78800-3_24,
author="de Moura, Leonardo
and Bj{\o}rner, Nikolaj",
editor="Ramakrishnan, C. R.
and Rehof, Jakob",
title="Z3: An Efficient SMT Solver",
booktitle="Tools and Algorithms for the Construction and Analysis of Systems",
year="2008",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="337--340",
abstract="Satisfiability Modulo Theories (SMT) problem is a decision problem for logical first order formulas with respect to combinations of background theories such as: arithmetic, bit-vectors, arrays, and uninterpreted functions. Z3 is a new and efficient SMT Solver freely available from Microsoft Research. It is used in various software verification and analysis applications.",
isbn="978-3-540-78800-3"
}

@TECHREPORT{BarFT-RR-17,
  author =	 {Clark Barrett and Pascal Fontaine and Cesare Tinelli},
  title =	 {{The SMT-LIB Standard: Version 2.6}},
  institution =	 {Department of Computer Science, The University of Iowa},
  year =	 2017,
  note =	 {Available at {\tt www.SMT-LIB.org}}
}
